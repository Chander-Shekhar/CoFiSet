{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Data Preprocessing\n",
    "data = pd.read_table('../dataset/u1base.txt', delim_whitespace=True, header = None, parse_dates=False)\n",
    "dataset=data.values[:,:2]\n",
    "feedback=data.values[:,2]\n",
    "users=data.values[:,0]\n",
    "observed_data=dataset[feedback>=3]\n",
    "d=10\n",
    "#Data Initialization\n",
    "Users=np.random.normal(0,1,(users[-1],d))\n",
    "Items=np.random.normal(0,1,(max(dataset[:,1]),d))\n",
    "Bias=np.random.normal(0,1,(max(dataset[:,1])))\n",
    "\n",
    "P_users=[]\n",
    "A_users=[]\n",
    "\n",
    "total_items=np.array(range(1,max(dataset[:,1])+1))\n",
    "for i in range(dataset[-1,0]):\n",
    "    P_users.append(dataset[users==i+1,1])\n",
    "    A_users.append(np.array([item for item in list(total_items) if item not in list(P_users[i])]))\n",
    "\n",
    "T=10\n",
    "learning_rate=0.1\n",
    "alpha_u=alpha_v=beta_v=0.1\n",
    "        \n",
    "perm=np.random.permutation(Users.shape[0])\n",
    "Users=Users[perm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_deriv(x):\n",
    "    sigm = 1. / (1. + np.exp(-x))\n",
    "    return sigm * (1. - sigm)\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    sigm = 1. / (1. + np.exp(-x))\n",
    "    return sigm\n",
    "\n",
    "def preference(user,P,A):\n",
    "    rp=0\n",
    "    for i in range(P.shape[0]):\n",
    "        rp+=np.dot(Users[user],Items[P[i]-1])+Bias[P[i]-1]\n",
    "        \n",
    "    ra=0\n",
    "    for i in range(A.shape[0]):\n",
    "        ra+=np.dot(Users[user],Items[A[i]-1])+Bias[A[i]-1]\n",
    "    \n",
    "    return rp/P.shape[0]-ra/A.shape[0]\n",
    "\n",
    "def calc_mean(d,P,A):\n",
    "    P_mean=np.zeros((d))\n",
    "    for i in P:\n",
    "        P_mean+=Items[i-1]\n",
    "    A_mean=np.zeros((d))\n",
    "    for i in A:\n",
    "        A_mean+=Items[i-1]\n",
    "    P_mean=P_mean/P.shape[0]\n",
    "    A_mean=A_mean/A.shape[0]\n",
    "    return P_mean,A_mean\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3067696082940943\n",
      "1.1466362426710817\n",
      "1.0402077292816614\n",
      "0.9887946144770517\n",
      "0.9124010939447226\n",
      "0.7908363942224447\n",
      "0.8659004243448561\n",
      "0.7924844622443751\n",
      "0.7253618845667706\n",
      "0.7228244722538317\n",
      "0.723507007145115\n",
      "0.7093339494046723\n",
      "0.7070290557548242\n",
      "0.6773200032763436\n",
      "0.6215749183819012\n",
      "0.64520119996752\n",
      "0.6558326399247237\n",
      "0.6740211937623475\n",
      "0.6407756239119082\n",
      "0.6212152380407749\n",
      "0.6427553374720033\n",
      "0.6127198716198509\n",
      "0.6214118229608494\n",
      "0.6158096771847039\n",
      "0.6427846557104513\n",
      "0.6201791573563782\n",
      "0.6263219413404729\n",
      "0.628979663326372\n",
      "0.6260443886102549\n",
      "0.6198766834401427\n",
      "0.6265658293566442\n",
      "0.6348814640217852\n",
      "0.6317402508704892\n",
      "0.6295041158779954\n",
      "0.6344221966679979\n",
      "0.6192351226260197\n",
      "0.6356915224274489\n",
      "0.6398471392827215\n",
      "0.628945803453082\n",
      "0.6298160746889289\n",
      "0.6321206563181156\n",
      "0.6280788407682266\n",
      "0.6226292495315562\n",
      "0.6341919669691659\n",
      "0.6234072572668518\n",
      "0.6259597050838446\n",
      "0.6226430124189012\n",
      "0.6262356898039688\n",
      "0.6258406233089726\n",
      "0.6275376267821833\n",
      "0.6312158313197203\n",
      "0.6242401933162404\n",
      "0.6209623734734666\n",
      "0.6210654081592131\n",
      "0.6162149778899014\n",
      "0.6179821667728753\n",
      "0.6154533392759077\n",
      "0.6133150242496418\n",
      "0.5978212453592644\n",
      "0.5981920236534513\n",
      "0.5926856815520368\n",
      "0.5807275328156762\n",
      "0.579859262184279\n",
      "0.5753696165151675\n",
      "0.579247017513457\n",
      "0.5725298716971812\n",
      "0.5592546837504337\n",
      "0.5595457727144331\n",
      "0.5550087026146635\n",
      "0.5415228567915756\n",
      "0.5424995737539965\n",
      "0.5387194242344981\n",
      "0.5368074717709335\n",
      "0.5294849684941707\n",
      "0.5152293477479171\n",
      "0.5169251712545629\n",
      "0.5070779823113737\n",
      "0.4990659572803737\n",
      "0.49307196893179916\n",
      "0.4948070149038459\n",
      "0.482099367992692\n",
      "0.47861804387206813\n",
      "0.4759764757842039\n",
      "0.4704880763317346\n",
      "0.4633819714353145\n",
      "0.4646244374392376\n",
      "0.4542725551876968\n",
      "0.4550412307650929\n",
      "0.45139047371723784\n",
      "0.4450437958139726\n",
      "0.4372326845065785\n",
      "0.43591219983047647\n",
      "0.4328189886687681\n",
      "0.43810477630799355\n",
      "0.4307346621829766\n",
      "0.42453895326814256\n",
      "0.41774852545900315\n",
      "0.41821989385988567\n",
      "0.4074479329920913\n",
      "0.41010573580446\n",
      "0.40080057464742436\n",
      "0.407444594823967\n",
      "0.397859985896446\n",
      "0.41340529339153353\n",
      "0.4034534908008374\n",
      "0.39653426271907444\n",
      "0.40605734163441703\n",
      "0.3950668355482496\n",
      "0.3921038295539266\n",
      "0.3892146983730892\n",
      "0.38256653346496544\n",
      "0.3867093124181155\n",
      "0.3923701151080136\n",
      "0.38257665962535037\n",
      "0.38584134138074766\n",
      "0.37316725039677484\n",
      "0.38168178732188607\n",
      "0.3840737824929569\n",
      "0.3689592445503659\n",
      "0.36969196140368765\n",
      "0.3790158478713834\n",
      "0.37355261163281056\n",
      "0.3784990564353421\n",
      "0.36457733682988636\n",
      "0.36733083049031684\n",
      "0.37317876690542884\n",
      "0.3760548799006104\n",
      "0.3800925412491571\n",
      "0.3667027051348096\n",
      "0.3749188096697808\n",
      "0.3647015431522633\n",
      "0.36386242524512774\n",
      "0.3739365591883871\n",
      "0.362300042842103\n",
      "0.36735994301548924\n",
      "0.3615136308662857\n",
      "0.36924555440878\n",
      "0.34933391295329763\n",
      "0.36822120825804305\n",
      "0.3641751680137964\n",
      "0.36842359339865\n",
      "0.36521780687431327\n",
      "0.3584990750994469\n",
      "0.36015213453725553\n",
      "0.35945053212714506\n",
      "0.3644290124798693\n",
      "0.35957137549898655\n",
      "0.3562197600713881\n",
      "0.3608177396594152\n",
      "0.3515067930401453\n",
      "0.356694384271785\n",
      "0.35700307096033185\n",
      "0.3649426716693439\n",
      "0.3610756169287736\n",
      "0.3599786427287959\n",
      "0.3630882133140494\n",
      "0.3595127994049138\n",
      "0.3564520133353253\n",
      "0.36942122238762776\n",
      "0.35749461037923397\n",
      "0.36474395534391657\n",
      "0.3601578580573771\n",
      "0.35527056759197495\n",
      "0.358380623577203\n",
      "0.3614449707461653\n",
      "0.36815367334736115\n",
      "0.35334602270035764\n",
      "0.3572150404102622\n",
      "0.3414402122686502\n",
      "0.3560408699584111\n",
      "0.35645529564198947\n",
      "0.34841951532317633\n",
      "0.34764874853782035\n",
      "0.3599505098855072\n",
      "0.35701896424395\n",
      "0.36082229544490596\n",
      "0.3622353774521541\n",
      "0.34881431949848185\n",
      "0.3522401250299543\n",
      "0.36087171972204557\n",
      "0.3478371201796787\n",
      "0.35176776752072725\n",
      "0.3517863724137041\n",
      "0.34604491988175234\n",
      "0.3554361216418483\n",
      "0.35116248648297316\n",
      "0.34770312776924117\n",
      "0.35876594136812345\n",
      "0.35802659845276164\n",
      "0.34687499419434836\n",
      "0.3602386352698315\n",
      "0.349244380800408\n",
      "0.3513303160362879\n",
      "0.3540390681508957\n",
      "0.35262884349800416\n",
      "0.3619381729574885\n",
      "0.3627996534407499\n",
      "0.34834551204897785\n",
      "0.3653256363164371\n",
      "0.3433937851977665\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "for t1 in range(200):\n",
    "    loss=0\n",
    "    for user in range(Users.shape[0]):\n",
    "        p_size=np.random.randint(d)+1\n",
    "        a_size=np.random.randint(p_size)+1\n",
    "        P_indices=np.random.randint(P_users[user].shape[0],size=p_size)\n",
    "        P=P_users[user][P_indices]\n",
    "        A_indices=np.random.randint(int(len(A_users[user])),size=a_size)\n",
    "        A=A_users[user][A_indices]\n",
    "        rpa=preference(user,P,A)\n",
    "        #loss\n",
    "        loss+=-np.log(sigmoid(rpa))\n",
    "        #derivate of loss fn with respect to R_upa\n",
    "        \n",
    "        del_rpa=-sigmoid(-rpa)\n",
    "        P_mean,A_mean=calc_mean(d,P,A)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #update equation\n",
    "        Users[user]-= learning_rate*((P_mean-A_mean)*del_rpa + alpha_u*Users[user])\n",
    "        for i in P:\n",
    "            Items[i-1]-= learning_rate*(del_rpa*Users[user]/P.shape[0] + alpha_v*Items[i-1])\n",
    "            \n",
    "        P_mean,A_mean=calc_mean(d,P,A)\n",
    "        Users[user]-= learning_rate*((P_mean-A_mean)*del_rpa + alpha_u*Users[user])\n",
    "        \n",
    "        for i in A:\n",
    "            Items[i-1]-= learning_rate*(-del_rpa*Users[user]/A.shape[0] + alpha_v*Items[i-1])\n",
    "        \n",
    "        P_mean,A_mean=calc_mean(d,P,A)\n",
    "        Users[user]-= learning_rate*((P_mean-A_mean)*del_rpa + alpha_u*Users[user])\n",
    "        \n",
    "        for i in P:\n",
    "            Bias[i-1]-= learning_rate*(del_rpa/P.shape[0] + beta_v*Bias[i-1] )\n",
    "        for i in A:\n",
    "            Bias[i-1]-= learning_rate*(del_rpa/A.shape[0] + beta_v*Bias[i-1] )\n",
    "    loss_epoch=loss/Users.shape[0]\n",
    "    print(loss_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "462\n",
      "943\n",
      "943\n",
      "0.032237539766701954\n"
     ]
    }
   ],
   "source": [
    "# Data Preprocessing\n",
    "data_test = pd.read_table('../dataset/u1test.txt', delim_whitespace=True, header = None, parse_dates=False)\n",
    "dataset_test=data_test.values[:,:2]\n",
    "feedback_test=data_test.values[:,2]\n",
    "users_test=data_test.values[:,0]\n",
    "observed_data_test=dataset_test[feedback_test>=3] \n",
    "P_users_test=[]\n",
    "for i in range(dataset_test[-1,0]):\n",
    "    P_users_test.append(dataset_test[users_test==i+1,1])\n",
    "\n",
    "print(len(P_users_test))\n",
    "print(Users.shape[0])\n",
    "\n",
    "K=5\n",
    "R=[]\n",
    "Ranking=[]\n",
    "for u in range(Users.shape[0]):\n",
    "    temp=[]\n",
    "    for i in range(Items.shape[0]):\n",
    "        temp.append(np.dot(Users[u],Items[i])+Bias[i])\n",
    "    Ranking.append(np.argsort(temp)[::-1])\n",
    "    R.append(np.sort(temp)[::-1])\n",
    "    \n",
    "print(len(Ranking))\n",
    "Precision=0\n",
    "for u in range(Users.shape[0]):\n",
    "    Precision_u=0\n",
    "    for i in range(K):\n",
    "        if(Ranking[u][i] in dataset_test[users_test==u,:]):\n",
    "            Precision_u+=1\n",
    "    Precision+=Precision_u/K\n",
    "Precision=Precision/Users.shape[0]\n",
    "print(Precision)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
